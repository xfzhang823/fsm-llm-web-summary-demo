# **FSM-Driven Web Summary Demo**

*A fully asynchronous, DuckDB-backed, finite-state-machine-controlled web scraping + LLM summarization pipeline.*

---

## **Overview**

This project demonstrates:

### âœ… **Fully traceable multi-stage pipelines**

Driven by an explicit, ordered FSM rail:

> **URL â†’ WEB_PAGE â†’ WEB_SUMMARY â†’ DONE**

Defined in `fsm_stage_config.py` and `pipeline_enums.py`.

### âœ… **DuckDB as the storage + control plane**

Four tables are defined in the central schema registry (`db_schema_registry.py`):

* `url`
* `pipeline_control`
* `web_page`
* `web_summary`

Each table has:

* a Pydantic model  
* explicit primary keys  
* authoritative column ordering  
* schema-aware ingestion and alignment  

### âœ… **Asynchronous, lease-aware processing**

All workers use:

* `try_claim_one()`
* `release_one()`
* `get_claimable_worklist()`

(from `db_utils.py`), ensuring:

* **only one worker processes a URL at a time**
* **crashed workers free leases automatically**
* **no stuck rows**

### âœ… **Two clear pipeline stages**

**Stage A: Web Scrape (URL â†’ WEB_PAGE)**

* Playwright-based async scraping  
* Text cleaning  
* Insert into `web_page`  

Stored in `web_scrape_pipeline_async_fsm.py`.

---

**Stage B: LLM Summarize (WEB_PAGE â†’ WEB_SUMMARY)**

* Load cleaned text  
* Send to OpenAI / Anthropic  
* Store JSON + text summary  

Stored in `web_summarize_pipeline_async_fsm.py`.

---

### âœ… **CLI Runner**

A rich command-line interface lets you run Stage A, Stage B, or both:

`cli/run_pipelines.py`

---

### âœ… **Complete Orchestrator**

For scripted one-shot runs, use:

`src/main.py`

---

# **Project Structure**

```text
fsm_llm_web_summary_demo/
â”œâ”€â”€ README.md
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ uv.lock
â”œâ”€â”€ logs/
â”œâ”€â”€ pipeline_data/
â”‚   â”œâ”€â”€ pipeline_data.duckdb                 # DuckDB database file
â”‚   â”œâ”€â”€ urls.json                            # Seed URLs for the pipeline
â”‚   â”œâ”€â”€ web_page.csv                         # Optional export/debug
â”‚   â””â”€â”€ web_summary.csv                      # Optional export/debug
â””â”€â”€ src/
    â”œâ”€â”€ main.py                              # High-level orchestrator entrypoint
    â”‚
    â”œâ”€â”€ cli/
    â”‚   â”œâ”€â”€ __init__.py
    â”‚   â””â”€â”€ run_pipelines.py                 # CLI wrapper (python -m cli.run_pipelines)
    â”‚
    â”œâ”€â”€ config/
    â”‚   â”œâ”€â”€ db_inserters.yaml                # YAML config for insert behaviors
    â”‚   â”œâ”€â”€ db_loaders.yaml                  # YAML config for table loaders
    â”‚   â”œâ”€â”€ logging_config.py                # Central logging setup
    â”‚   â””â”€â”€ project_config.py                # Project root, DuckDB paths, LLM defaults
    â”‚
    â”œâ”€â”€ db_io/
    â”‚   â”œâ”€â”€ create_db.py                     # (Legacy/simple) DB creation helper
    â”‚   â”œâ”€â”€ create_db_tables.py              # Idempotent DDL for all tables
    â”‚   â”œâ”€â”€ db_inserters.py                  # Schema-aligned INSERT logic (YAML-driven)
    â”‚   â”œâ”€â”€ db_loaders.py                    # Table loaders using db_loaders.yaml
    â”‚   â”œâ”€â”€ db_schema_registry.py            # SST registry: models, PKs, column order
    â”‚   â”œâ”€â”€ db_transform.py                  # add_metadata + modelâ†’DataFrame transforms
    â”‚   â”œâ”€â”€ db_utils.py                      # Worklist, leases, FSM helpers, queries
    â”‚   â”œâ”€â”€ file_ingestion.py                # urls.json â†’ UrlFile â†’ url table
    â”‚   â”œâ”€â”€ get_db_connection.py             # Central DuckDB connection factory
    â”‚   â””â”€â”€ persist_pipeline_state.py        # Persist/refresh pipeline_control from FSM
    â”‚
    â”œâ”€â”€ fsm/
    â”‚   â”œâ”€â”€ fsm_stage_config.py              # Ordered rail: URL â†’ WEB_PAGE â†’ WEB_SUMMARY â†’ DONE
    â”‚   â”œâ”€â”€ pipeline_enums.py                # PipelineStage/Status/TaskState, LLMProvider, TableName
    â”‚   â”œâ”€â”€ pipeline_fsm.py                  # Per-URL FSM implementation
    â”‚   â”œâ”€â”€ pipeline_fsm_manager.py          # Manager: load/store FSM from pipeline_control
    â”‚   â””â”€â”€ state_sync.py                    # Sync FSM â†” pipeline_control rows
    â”‚
    â”œâ”€â”€ llm_api/
    â”‚   â”œâ”€â”€ llm_api_utils_async.py           # Async wrappers for OpenAI/Anthropic APIs
    â”‚   â”œâ”€â”€ llm_response_validators.py       # Validation â†’ JSONResponse/TextResponse/etc
    â”‚   â””â”€â”€ prompt_templates_web.py          # Web summarization prompt templates
    â”‚
    â”œâ”€â”€ models/
    â”‚   â”œâ”€â”€ db_loaders_config_models.py      # Pydantic models for loader YAML config
    â”‚   â”œâ”€â”€ db_table_models.py               # Pydantic models for DuckDB tables
    â”‚   â”œâ”€â”€ llm_response_models.py           # JSONResponse, TextResponse, TabularResponse, CodeResponse
    â”‚   â”œâ”€â”€ pydantic_model_loaders_for_files.py
    â”‚   â”œâ”€â”€ url_file_models.py               # Url, UrlFile (seed JSON format)
    â”‚   â””â”€â”€ web_content_models.py            # WebPageContent, WebPageBatch, WebSummaryJSON
    â”‚
    â”œâ”€â”€ pipelines/
    â”‚   â”œâ”€â”€ advance_completed_to_next_stage.py
    â”‚   â”œâ”€â”€ sync_url_table_to_pc_pipeline_fsm.py
    â”‚   â”œâ”€â”€ update_urls_pipeline_fsm.py
    â”‚   â”œâ”€â”€ urls_pipeline_fsm.py
    â”‚   â”œâ”€â”€ web_scrape_pipeline_async_fsm.py
    â”‚   â”œâ”€â”€ web_summarize_pipeline_async_fsm.py
    â”‚   â””â”€â”€ webpage_reader_async.py
    â”‚
    â”œâ”€â”€ utils/
    â”‚   â””â”€â”€ find_project_root.py
    â”‚
    â””â”€â”€ fsm_llm_web_summary_demo.egg-info/
````

---

# **DuckDB Schema**

Managed centrally via **`DUCKDB_SCHEMA_REGISTRY`**.

| Table              | Purpose                              | PK                                       |
| ------------------ | ------------------------------------ | ---------------------------------------- |
| `url`              | Canonical URL registry               | `url`                                    |
| `pipeline_control` | FSM state, machine lease, human gate | `url, iteration`                         |
| `web_page`         | Cleaned webpage text                 | `url, iteration`                         |
| `web_summary`      | LLM output (text + JSON + metadata)  | `url, iteration, llm_provider, model_id` |

All schema â†’ strict Pydantic models in `db_table_models.py`.

### **Note on Token Usage Fields**

The `web_summary` table includes three optional columns related to LLM token usage:

* `tokens_prompt`
* `tokens_completion`
* `tokens_total`

These fields are present for **future expansion**, but the current demo does **not** extract or store token usage from OpenAI or Anthropic responses.
As a result, these columns will appear as `NULL` in DuckDB.

This simplifies the LLM wrapper code and keeps the demo focused on the core FSM-driven scraping + summarization workflow.

---

# **Quick Start**

This section shows how to run the **entire FSM-driven DuckDB pipeline** â€” from URL ingestion â†’ webpage scraping â†’ LLM summaries â€” in just a few commands.

---

## **1. Install Dependencies**

From the project root:

```bash
uv sync
```

Or, using pip:

```bash
pip install -e .
```

Install Playwright browsers:

```bash
playwright install
```

---

## **2. Prepare Your URL Seed File**

Edit:

```
pipeline_data/urls.json
```

Format is flexible:

```json
{
  "urls": [
    {
      "url": "https://example.com",
      "source": "manual",
      "tags": ["demo"]
    }
  ]
}
```

Or simply:

```json
[
  { "url": "https://example.com" },
  { "url": "https://wikipedia.org" }
]
```

---

## **3. Run the Full Pipeline (Scrape â†’ Summarize)**

```bash
python -m cli.run_pipelines all
```

This performs:

1. Creates all DuckDB tables (idempotent).
2. Ingests `urls.json` â†’ `url` table.
3. Syncs `url` â†’ `pipeline_control`.
4. **Stage A:** Scrapes and cleans webpages.
5. **Stage B:** Summarizes via OpenAI/Anthropic.

Outputs stored in:

```
pipeline_data/pipeline_data.duckdb
```

---

## **4. View the Results**

```bash
duckdb pipeline_data/pipeline_data.duckdb
```

Then inside DuckDB:

```sql
SELECT * FROM web_page LIMIT 10;
SELECT * FROM web_summary LIMIT 10;
SELECT url, summary_text FROM web_summary ORDER BY created_at DESC;
```

---

## **5. Run Stages Individually**

### Stage A only:

```bash
python -m cli.run_pipelines scrape --max 5
```

### Stage B only:

Using OpenAI:

```bash
python -m cli.run_pipelines summarize --provider openai --model gpt-4.1-nano
```

Using Anthropic:

```bash
python -m cli.run_pipelines summarize --provider anthropic --model claude-3-haiku
```

Retry failed rows:

```bash
python -m cli.run_pipelines summarize --retry-errors
```

---

## **6. Advance Completed Rows**

```bash
python -m cli.run_pipelines advance-completed
```

---

## **7. Logs and Debugging**

All logs written to:

```
logs/<username>_YYYY-MM-DD_HH-MM-SS_app.log
```

Look for:

* Scrape errors
* Summarization errors
* FSM transitions
* Lease expirations

---

# **LLM Behavior**

Stage B uses structured JSON-based summarization via:

* `SUMMARIZE_WEBPAGE_TO_JSON_PROMPT`
* `JSONResponse` (in `llm_response_models.py`)
* `WebSummaryJSON` (validated JSON structure)

Human-readable text is synthesized via `_derive_summary_text`.

---

# **Lease / FSM Logic**

Provided by:

* `get_claimable_worklist`
* `try_claim_one`
* `renew_lease`
* `release_one`

FSM transitions follow the rail in `fsm_stage_config.py`.

Each stage pipeline enforces:

* Stage-gate checks
* Lease semantics
* `fsm.step()` after success

---

# **Configuration**

All configs live in `config/project_config.py`.

Key fields:

* `DUCKDB_FILE`
* `URL_JSON_FILE`
* `GPT_4_1_NANO`, `CLAUDE_HAIKU`
* Provider constants `{OPENAI, ANTHROPIC}`

---

# **Extending the Pipeline**

You can extend this architecture to:

* Add new pipeline stages
* Add new DuckDB tables
* Add batch LLM processing
* Run parallel workers
* Integrate background jobs

To add a new stage:

1. Add a `PipelineStage` enum
2. Add it to `PIPELINE_STAGE_SEQUENCE`
3. Create a new stage pipeline file
4. Add a table + Pydantic model
5. Register it in `DUCKDB_SCHEMA_REGISTRY`

Everything else â€œjust works.â€

---

# **Appendix: Key Design Principles**

### **Single Source of Truth**

All schema definitions and column ordering live in `db_schema_registry.py`.

### **Strict Pydantic Validation**

Every DB row is validated on read/write.

### **Idempotency**

Re-running stages is safe via dedup logic in `db_inserters.py`.

### **Concurrency + Fault Tolerance**

Async workers + leases prevent double work; expired leases allow recovery.

### **Separation of Responsibilities**

Stage A = scraping & cleaning
Stage B = summarization

---

# **Appendix: Architecture Diagrams**

## **1. High-Level FSM Pipeline Overview**

```mermaid
flowchart LR
    A[URL Stage] -->|Scrape webpages| B[WEB_PAGE Stage]
    B -->|LLM Summaries| C[WEB_SUMMARY Stage]
    C -->|Complete| D[DONE]

    subgraph FSM Rail
        A
        B
        C
        D
    end
```

---


## **2. Full System Architecture (End-to-End)**

```mermaid
flowchart TD
    subgraph Input["ğŸ“¥ Inputs"]
        URLFile["urls.json"]
    end

    subgraph Ingestion["ğŸ—‚ï¸ URL Ingestion"]
        UF[UrlFile <br/> Pydantic Validation]
        UP[URL â†’ url table]
    end

    subgraph DB["ğŸ¦† DuckDB (Structured Tables)"]
        T1[url table]
        T2[pipeline_control<br/>(FSM state + lease info)]
        T3[web_page table]
        T4[web_summary table]
    end

    subgraph StageA["ğŸŒ Stage A: Scrape (URL â†’ WEB_PAGE)"]
        CA[get_claimable_worklist]
        TC[try_claim_one]
        FW[fetch_and_persist_webpage_async]
        PC1[release_one + fsm.step()]
    end

    subgraph StageB["ğŸ§  Stage B: Summarize (WEB_PAGE â†’ WEB_SUMMARY)"]
        CB[get_claimable_worklist]
        TC2[try_claim_one]
        LB[Load clean_text]
        LLM[LLM API]
        WR[WebSummaryRow]
        PC2[release_one + fsm.step()]
    end

    subgraph CLI["ğŸ–¥ï¸ CLI + Orchestrator"]
        CLI1["python -m cli.run_pipelines"]
        ALL[run_all_fsm()]
    end

    URLFile --> UF --> UP --> T1 --> T2

    %% Stage A
    T2 --> CA --> TC --> FW --> T3 --> PC1 --> T2

    %% Stage B
    T2 --> CB --> TC2 --> LB --> LLM --> WR --> T4 --> PC2 --> T2

    CLI1 --> ALL
    ALL --> UP
    ALL --> StageA
    ALL --> StageB
```

---

## **3. DuckDB Schema + Pydantic Model Layer**

```mermaid
classDiagram
    class UrlRow {
        url str
        source str?
        tags str?
        created_at datetime
        updated_at datetime
    }

    class PipelineState {
        url str
        iteration int
        stage PipelineStage
        status PipelineStatus
        task_state PipelineTaskState
        is_claimed bool
        worker_id str?
        lease_until datetime?
        decision_flag int?
        transition_flag int
        notes str?
        source_file str?
        created_at datetime
        updated_at datetime?
    }

    class WebPageRow {
        url str
        iteration int
        clean_text str?
        html str?
        status_code int?
        fetched_at datetime?
        fetch_error str?
        created_at datetime
        updated_at datetime?
    }

    class WebSummaryRow {
        url str
        iteration int
        summary_text str?
        summary_json dict?
        llm_provider str
        model_id str?
        tokens_prompt int?
        tokens_completion int?
        tokens_total int?
        created_at datetime
        updated_at datetime?
    }

    UrlRow --> PipelineState
    PipelineState --> WebPageRow
    WebPageRow --> WebSummaryRow
```

---

## **4. Lease + Human-Gate State Machine**

```mermaid
stateDiagram-v2
    [*] --> NEW

    NEW --> IN_PROGRESS : try_claim_one()
    IN_PROGRESS --> COMPLETED : release_one(COMPLETED)
    IN_PROGRESS --> ERROR : release_one(ERROR)

    ERROR --> NEW : retry (if retry_errors=True)
    ERROR --> SKIPPED : human sets SKIP
    PAUSED --> READY : human unlocks
    READY --> IN_PROGRESS : claim again

    state IN_PROGRESS {
        [*] --> claimed
        claimed --> expired : lease_until < now
        expired --> claimed : re-claimable
    }
```

---

## **5. Stage A (Async Web Scraping)**

```mermaid
sequenceDiagram
    autonumber
    participant PC as pipeline_control
    participant Worker as Stage A Worker
    participant Playwright
    participant DB as DuckDB

    Worker->>PC: get_claimable_worklist(URL)
    Worker->>PC: try_claim_one(url, iteration)
    PC-->>Worker: lease acquired

    Worker->>Playwright: goto(url)
    Playwright-->>Worker: clean_text

    Worker->>DB: INSERT WebPageRow
    Worker->>PC: release_one(COMPLETED)
    Worker->>PC: fsm.step()
```

---

## **6. Stage B (LLM Summarization)**

```mermaid
sequenceDiagram
    autonumber
    participant PC as pipeline_control
    participant Worker as Stage B Worker
    participant DB as DuckDB
    participant LLM as OpenAI/Anthropic

    Worker->>PC: get_claimable_worklist(WEB_PAGE)
    Worker->>PC: try_claim_one(url, iteration)
    PC-->>Worker: lease granted

    Worker->>DB: SELECT clean_text
    DB-->>Worker: clean_text

    Worker->>LLM: summarize(clean_text)
    LLM-->>Worker: summary_json

    Worker->>DB: INSERT WebSummaryRow
    Worker->>PC: release_one(COMPLETED)
    Worker->>PC: fsm.step()
```

---

## **7. Control-Plane Interaction**

```mermaid
flowchart TD
    A[Row in pipeline_control] --> B{stage == expected?}
    B -- Yes --> C[Process]
    B -- No --> D[release_one(ERROR)]
    D --> E[Skipped / left for human]
```

---

## **8. CLI Execution Flow**

```mermaid
flowchart LR
    CLI["python -m cli.run_pipelines"] --> CMD{Command}

    CMD -->|scrape| S[run_web_scrape_pipeline_async_fsm()]
    CMD -->|summarize| Z[run_web_summary_pipeline_async_fsm()]
    CMD -->|all| A[create tables]

    A --> B[run_update_urls_pipeline_fsm]
    B --> C[run_sync_url_table_to_pc_pipeline_fsm]
    C --> D[run_urls_pipeline_fsm]
    D --> S
    S --> Z
```

---

# **License**

MIT License
